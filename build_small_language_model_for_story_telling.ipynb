{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpQnsyKiOWO1Bgi1fYDVVN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
 }
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rikijha/slm/blob/main/build_small_language_model_for_story_telling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "I have build a Small Language Model (SLM) from scratch. I have kept the parameter size to 50-60 million.\n",
        "\n",
        "Goal here is to generate creative and coherent text based on the input data."
      ],
      "metadata": {
        "id": "EQ1ZNbnAlASX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 : Import the Dataset**\n",
        "\n",
        "TinyStories is a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We can get it from HuggingFace."
      ],
      "metadata": {
        "id": "UOgzMJOYoiwI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAsv6wGD7vQN",
        "outputId": "f796e976-23d6-4ba1-df9e-25708699ce17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436,
          "referenced_widgets": [
            "c40f704ef2b3406d90f6af963e2c0d2e",
            "b61a68e9abfa4a928c2d8686013187f9",
            "84c8f6e1fec44916b2e620ef40205c30",
            "582119a580794996b9e34535d9e81d66",
            "a3aeb1583db6451eb470aa0ffb13c6a8",
            "db8aba2cf84e45379c092e838564bcdd",
            "d0a448945dae42d38c2b9f53c2ebda2f",
            "0dca2699da9e47fe8581dd832799e877",
            "0ba3359bc62d4af5845c8db6417d15e8",
            "be5fd3bb44a14d0cac8d1d6e81a63f0a",
            "e4c8af6d29b442b4b0f2dbb78c05ddad",
            "d0ea669b88544a39a772d8ca23a88b6f",
            "22baae75167548e1aebfa96945ab5497",
            "5cb6fc0d52074df4ad51d87e5461ad54",
            "edbb44a891014ca8aa6a4d6b8b5faf92",
            "247ff777c58f4b899a2d7b75ec63752f",
            "2b604dc0e5e6403191c71d2b2ce663d9",
            "9fa3df4fe37049eab6126d154dfa2590",
            "ffc11d6bf3974c9fa5fa4c1bbc38cac8",
            "3efc9de3bffa4c1dac9df30da42ecec9",
            "59e4948fccfa4f5dbef50be963610a07",
            "db6bc31c688e443f956dc9f0d179aa0e",
            "3a498ba676e8483083c9b24515d557b6",
            "325e851db621423d91eea3e3436aac79",
            "885fdb18ccd84020bcfc006a523af394",
            "dafa429bf9d54d558c7d8542c4be2b44",
            "2a015dc0eb2b46de98991a5c65224b2d",
            "ef2c3f6c65a242a099f0f0f54733c5af",
            "9a867757afca46cda5c280c36b481da7",
            "27f4e6bed008426ca552085a985fe86f",
            "56b29fbf7d174f2da77bfb6955f15425",
            "8f0fb45eda8b42f38c645a68baa38619",
            "48de63ec91c44450a9eb5efc037cd549",
            "820bcd2caaa14a34ac96efcf669a07bb",
            "13eeab862f2e48d7be3e519f303cfaa0",
            "89591d60ddd342a882e6d98e94e2bb15",
            "7907adbdef014efeb6d7eaa005992e95",
            "ca4122d8c7f047ea884524e419b9a4a2",
            "16dcd2be2c77440280b029c71ef89a30",
            "ad3565cbf31d4812972f3b225b36aca9",
            "1cfdff95189241aca122cd0c5a5af78b",
            "656cad39e4fe40eda7373962b945d1d1",
            "1085b85200cb4882bca357c13fcc2620",
            "2e3274098c274abaa2368adaeed68fb2",
            "fbc6ac4a9e574b1b9167863da3302c86",
            "bbec937967dd4d7bb539e6ee66854e0d",
            "88260e12a42946aca5582b829df8c870",
            "ae963151acae4a9c81fb1ce353967579",
            "6eadc94be39e4baaa72e7c78fa82ad8a",
            "094211f1ba1b46d5928cd78d743950bf",
            "bbf0adbcfccb4dae8d43b445215077da",
            "f21e102e81fc44158edbb5b5bd31112c",
            "44721c000eb142c1a9909c404e2c1c94",
            "c9441ce8f4cf41a1b868798d51fe2437",
            "55cdd09a4b6b4dc2bb232050a4d153b4",
            "381a8929d15b4d40abaacfeb376c858c",
            "1cf30924ff3a4fce9856c3833c54365d",
            "0162aa13af254e7d88c5ed1ff265cf34",
            "df6fea31db1f4e908e63c510ab4b8e2e",
            "83499f8b5b8a4ce284b44281c151b2ba",
            "f574f8fc8c58469eaa96b3fa7324966d",
            "a897d8fe700c4a1bb5336d2b00b1eb73",
            "62f4fe011d034835bd3ab6810c5761ae",
            "08b62354d5944252bd14cf365e01bf53",
            "b1ba62e3d8fc44588884181433bfbc3e",
            "3c1afba6ab6f42a8ba976e69350ddf1d",
            "6537d20f0d46427e848dd3c1e1f772e4",
            "5512e61e1754425aa3b4db002a84408a",
            "7bb4a652eda64365a22f3ce7fe4e26c9",
            "630aa7b601bb4567b660cf28450f212a",
            "7bac5f4f28724ae197f354197ee6c964",
            "6bc5affd7f95485f9d6670f4c813e74d",
            "8a16765dad4f4e1ab9f8e57e7ea3a5f7",
            "3239324bc94b4264a3d9ef47b1f56663",
            "9f80b76f0e7c442081b9435cc9435770",
            "e7f43b0088ca49259c787d39e27470dd",
            "7c6efe4ca98d4c5f932c15a929765358",
            "b9172b2b8449406cbf98a6f579726405",
            "8e0c9b7aa5594843afb1c657f962ab06",
            "23187042fdb042b58f50e930881d3ac1",
            "cfe0c34635cd43c49da55975ecefeea5",
            "bbc486cdd45c411f922026d61914e4f2",
            "3f8667c89a6544bd945f3406de65a353",
            "a84686400a4e4fa29aea2d2f7cbabdc3",
            "dd192a5fbfa948b3a81f2fb602b8b603",
            "130495361da447988ea44ab5a99b9ad6",
            "645ad558e33a4891a887a66e34072cfa",
            "f6d28c8987404b42a47a5a64ec14790d"
          ]
        },
        "id": "aIXDMpMq8IxI",
        "outputId": "daa792f6-e89b-4c34-b1d8-5a58e28bb4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c40f704ef2b3406d90f6af963e2c0d2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00004-2d5a1467fff108(…):   0%|          | 0.00/249M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0ea669b88544a39a772d8ca23a88b6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00001-of-00004-5852b56a2bd28f(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a498ba676e8483083c9b24515d557b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00002-of-00004-a26307300439e9(…):   0%|          | 0.00/246M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "820bcd2caaa14a34ac96efcf669a07bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00003-of-00004-d243063613e5a0(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbc6ac4a9e574b1b9167863da3302c86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/validation-00000-of-00001-869c898b5(…):   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "381a8929d15b4d40abaacfeb376c858c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6537d20f0d46427e848dd3c1e1f772e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9172b2b8449406cbf98a6f579726405"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Tokenize the Dataset\n",
        "\n",
        "(1) Tokenize the dataset into tokenIDs.\n",
        "\n",
        "(2) Created a file called \"train.bin\" and \"validtion.bin\" to  store the tokenIDs from the entire dataset.\n",
        "\n",
        "(3) The tokenIDs are stored on a disk, rather than on the RAM for efficient computations."
      ],
      "metadata": {
        "id": "NFKiYWODo8Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n",
        "\n",
        "def process(example):\n",
        "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
        "    out = {'ids': ids, 'len': len(ids)}\n",
        "    return out\n",
        "\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "    tokenized = ds.map(\n",
        "        process,\n",
        "        remove_columns=['text'],\n",
        "        desc=\"tokenizing the splits\",\n",
        "        num_proc=8,\n",
        "        )\n",
        "    # concatenate all the ids in each dataset into one large file we can use for training\n",
        "    for split, dset in tokenized.items():\n",
        "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "        filename = f'{split}.bin'\n",
        "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
        "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "        total_batches = 1024\n",
        "\n",
        "        idx = 0\n",
        "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
        "            # Batch together samples for faster write\n",
        "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "            arr_batch = np.concatenate(batch['ids'])\n",
        "            # Write into mmap\n",
        "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "            idx += len(arr_batch)\n",
        "        arr.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "43c8d616646f46a894c0310ea948926e",
            "c8a288872c66430a83393f62208ead24",
            "c98457ffe628406982713ebd0839a91f",
            "8045f26493744bc795f492baa2e23182",
            "d48d0b1626c24d79a8578eeda2fa1e5c",
            "0f115dda715642fa88a99a9962853868",
            "be6643c330b1402fb8381d475900babd",
            "aafe0bb270094abbae0e06023d0792f3",
            "e447d4c0d0f84f8d9f02846117016744",
            "4f242b39ba2c4833bec8bc7ef8986e11",
            "9942494541d342f59005e541b52d5599",
            "0c440a10b11448c5ba28f56827b77064",
            "a66ea64bfffe450183142fd25d14059b",
            "6cb289f342d34ff4acfb4f0c7d48e2a2",
            "f610c08c0db944628dce917c670c74d4",
            "a047d2eddd3d47769e685aa7c691acfd",
            "5d7ecdf3046e4aa29ddaf98bd8cc1596",
            "1e851f824eee4e33bb66142876995012",
            "07d0fa300d6c458eaf3749f3cabeffcb",
            "860404619af143a7b4ebcca0740c0f55",
            "409f0387881447a396f2e11dda50f721",
            "18b9d334eb7a443eb68b00fd84bddfe3",
            "b1f7e7f7c3454f8faa68f068168a80e8",
            "7b8d0786f3024138b38c8cc7b0adcbe4",
            "2a5e8fad129345d1aaf93f7c17de720c",
            "c42327b9f1284aadb1bf1803fd91f895",
            "9095135bc5034243a4ac83043dd5c40a",
            "cde023733afe4b5cb7f964f0ea2e99fa",
            "1ec5bd9c946f45fb97e0b68c8c845e2d",
            "d192dd7e19c64391801b6aeb8d25d70b",
            "7ec6122586eb4dad98341dc75c2b98db",
            "bb86cc68d98740ca9a44b5ebf3b461c8",
            "7b921a1b8ca847d590f9a1ee0eb14b0d",
            "a2afb57f0e9a4ce89ef702a1fa6b4d52",
            "e24d705bf59d4825828b20577e3bf4b9",
            "14c94ee32d7844d9aff180e91535f6f3",
            "838704728f574cc9a08f80dd5b2b710e",
            "6bea903036214f54bb8ed29f22f3d394",
            "c3180d1d1f2444999a085a384840a21b",
            "bedc27d3a9504560908c4ef95c471ff0",
            "95894865658945cb80e407892d4842f0",
            "4e9e4566545849e3913c0c8b171067ae",
            "1007addff57744e8a913e047e2179fcd",
            "dc65617ecfda44489a487457d10d3408"
          ]
        },
        "id": "6pcctjabw3SF",
        "outputId": "99770d46-d880-4d65-949c-0a18952af0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizing the splits (num_proc=8):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43c8d616646f46a894c0310ea948926e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizing the splits (num_proc=8):   0%|          | 0/21990 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c440a10b11448c5ba28f56827b77064"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "writing train.bin:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1f7e7f7c3454f8faa68f068168a80e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "writing validation.bin:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2afb57f0e9a4ce89ef702a1fa6b4d52"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**: Create Input-Output batches for the dataset"
      ],
      "metadata": {
        "id": "CxMekAm6psYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
        "#block size = context window\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "yMYl2u5n3AX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Define the SLM Model Architecture"
      ],
      "metadata": {
        "id": "ccmM3HdTpw2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                       .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop=nn.Dropout(config.dropout),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            return logits, None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate tokens given a conditioning sequence.\n",
        "        idx: Tensor of shape (B, T)\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "1RBKn3VD2I8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig(\n",
        "    vocab_size=50257,     # use the tokenizer's vocab size\n",
        "    block_size=128,       # or whatever context size you're training with\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "model = GPT(config)"
      ],
      "metadata": {
        "id": "D5DIxuI82X8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5**: Define the loss function"
      ],
      "metadata": {
        "id": "VhDHvUxzp7EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "4O8AOEeK2cr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6**: Define SLM Training Configuration"
      ],
      "metadata": {
        "id": "oLAiOoTbqG9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Config\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "\n",
        "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
        "max_iters = 20000 #increase from 25000\n",
        "warmup_steps = 1000 #smoother initial train, earlier 100\n",
        "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
        "eval_iters = 500 # increased from 100\n",
        "batch_size = 32 # changed from 16, better gradient estimate\n",
        "block_size = 128 #changed from 64, capture longer range dependencies\n",
        "\n",
        "gradient_accumulation_steps = 32 # reduced from 50\n",
        "\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "\n",
        "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
        "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVkEaPDm2gjO",
        "outputId": "b2b84e07-52e0-43a2-9651-a87f7589ab23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cfaaa1cd6b0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Defining optimizers"
      ],
      "metadata": {
        "id": "YXUFWx_IqSxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "\n",
        "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
        "\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxugte_U2lFq",
        "outputId": "5fff9901-55a6-4f12-93d7-e3dbf1269725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2132813893.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8**: Pre-train the SLM"
      ],
      "metadata": {
        "id": "4-CD9PxsqfVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "model = model.to(device)\n",
        "\n",
        "# In your training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch % eval_iters == 0 and epoch != 0:\n",
        "        # Ensure estimate_loss uses the correct device\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
        "        train_loss_list += [losses['train']]\n",
        "        validation_loss_list += [losses['val']]\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    # Ensure X and y are on the correct device\n",
        "    X, y = get_batch(\"train\")\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X, y)\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b0f408b164fb4d9990f795b12fde5852",
            "cc8969ecb79f4a20a81a3358018579c2",
            "8da1a4aef56146f58e9d850e85935ea1",
            "a4f202fe74cb4cb3a8314955092f8a72",
            "c9e460881b5d41b4a1f2efdcc05481a1",
            "3d2de5353d254820bd886ceefe5c37f0",
            "4e885af297594267863d6063bac2d024",
            "2bf5e5abaeeb40a88e2db60b2f106612",
            "ef7c45949c674329ab95b08cf2970775",
            "0842497bd19849ac83791bb7a42b0d69",
            "e752d099f6884a9789f8a8fb5470bcb5"
          ]
        },
        "id": "WH2qG1Fb2r7Z",
        "outputId": "0c1f52ae-d072-45ed-e602-678f759419bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0f408b164fb4d9990f795b12fde5852"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500: train loss 9.4502, val loss 9.4545\n",
            "The current learning rate: 0.00007\n",
            "Epoch 1000: train loss 8.5002, val loss 8.5027\n",
            "The current learning rate: 0.00010\n",
            "Epoch 1500: train loss 7.5493, val loss 7.5458\n",
            "The current learning rate: 0.00010\n",
            "Epoch 2000: train loss 6.6954, val loss 6.6893\n",
            "The current learning rate: 0.00010\n",
            "Epoch 2500: train loss 5.9996, val loss 5.9922\n",
            "The current learning rate: 0.00011\n",
            "Epoch 3000: train loss 5.4960, val loss 5.4898\n",
            "The current learning rate: 0.00011\n",
            "Epoch 3500: train loss 5.0634, val loss 5.0614\n",
            "The current learning rate: 0.00012\n",
            "Epoch 4000: train loss 4.7393, val loss 4.7379\n",
            "The current learning rate: 0.00012\n",
            "Epoch 4500: train loss 4.4917, val loss 4.4942\n",
            "The current learning rate: 0.00013\n",
            "Epoch 5000: train loss 4.2703, val loss 4.2747\n",
            "The current learning rate: 0.00014\n",
            "Epoch 5500: train loss 4.1212, val loss 4.1204\n",
            "The current learning rate: 0.00015\n",
            "Epoch 6000: train loss 3.9783, val loss 3.9745\n",
            "The current learning rate: 0.00016\n",
            "Epoch 6500: train loss 3.8262, val loss 3.8291\n",
            "The current learning rate: 0.00018\n",
            "Epoch 7000: train loss 3.7121, val loss 3.7100\n",
            "The current learning rate: 0.00019\n",
            "Epoch 7500: train loss 3.6023, val loss 3.6029\n",
            "The current learning rate: 0.00020\n",
            "Epoch 8000: train loss 3.5117, val loss 3.5157\n",
            "The current learning rate: 0.00022\n",
            "Epoch 8500: train loss 3.4230, val loss 3.4235\n",
            "The current learning rate: 0.00024\n",
            "Epoch 9000: train loss 3.3322, val loss 3.3320\n",
            "The current learning rate: 0.00025\n",
            "Epoch 9500: train loss 3.2592, val loss 3.2661\n",
            "The current learning rate: 0.00027\n",
            "Epoch 10000: train loss 3.1768, val loss 3.1912\n",
            "The current learning rate: 0.00028\n",
            "Epoch 10500: train loss 3.1146, val loss 3.1202\n",
            "The current learning rate: 0.00030\n",
            "Epoch 11000: train loss 3.0518, val loss 3.0566\n",
            "The current learning rate: 0.00032\n",
            "Epoch 11500: train loss 3.0040, val loss 3.0067\n",
            "The current learning rate: 0.00033\n",
            "Epoch 12000: train loss 2.9402, val loss 2.9487\n",
            "The current learning rate: 0.00035\n",
            "Epoch 12500: train loss 2.8950, val loss 2.8957\n",
            "The current learning rate: 0.00036\n",
            "Epoch 13000: train loss 2.8491, val loss 2.8456\n",
            "The current learning rate: 0.00038\n",
            "Epoch 13500: train loss 2.8041, val loss 2.8092\n",
            "The current learning rate: 0.00040\n",
            "Epoch 14000: train loss 2.7651, val loss 2.7661\n",
            "The current learning rate: 0.00041\n",
            "Epoch 14500: train loss 2.7226, val loss 2.7194\n",
            "The current learning rate: 0.00042\n",
            "Epoch 15000: train loss 2.6906, val loss 2.6957\n",
            "The current learning rate: 0.00044\n",
            "Epoch 15500: train loss 2.6279, val loss 2.6397\n",
            "The current learning rate: 0.00045\n",
            "Epoch 16000: train loss 2.5942, val loss 2.5949\n",
            "The current learning rate: 0.00046\n",
            "Epoch 16500: train loss 2.5722, val loss 2.5690\n",
            "The current learning rate: 0.00047\n",
            "Epoch 17000: train loss 2.5412, val loss 2.5347\n",
            "The current learning rate: 0.00048\n",
            "Epoch 17500: train loss 2.5040, val loss 2.5077\n",
            "The current learning rate: 0.00048\n",
            "Epoch 18000: train loss 2.4754, val loss 2.4714\n",
            "The current learning rate: 0.00049\n",
            "Epoch 18500: train loss 2.4490, val loss 2.4555\n",
            "The current learning rate: 0.00049\n",
            "Epoch 19000: train loss 2.4146, val loss 2.4234\n",
            "The current learning rate: 0.00050\n",
            "Epoch 19500: train loss 2.4029, val loss 2.4005\n",
            "The current learning rate: 0.00050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9:** Plot the SLM Loss Function"
      ],
      "metadata": {
        "id": "yNCDgXtcqiay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
        "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
        "\n",
        "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
        "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
        "plt.xlabel(\"Steps - Every 100 epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "dHxXUyfVkPOT",
        "outputId": "b0b58633-590e-4366-f611-fb345c14e58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10:** Run SLM Inference on our trained model to test the output"
      ],
      "metadata": {
        "id": "9WJmB0MKqs9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model\n",
        "model = GPT(config)  # re-create the model with same config\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TuLBysckVal",
        "outputId": "a1498ebe-157e-4559-f2c2-7405977a4dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Once upon a time there was a pumpkin.\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbc12n6MkcmM",
        "outputId": "2df5bbd7-fd26-48e6-83a5-2d9b748da682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time there was a pumpkin. It was nice on much adventure. One day, the flower would go for a walk. The figure looked around and saw something put on the lamb. It looked like a big brick, he could find and it was a lunch full of fruits.\n",
            "\n",
            "On the way, the girl found a coin! It said, \"Do you want to peek now! It's so pretty!\"\n",
            "\n",
            "The girl was so excited. She wanted a closer bite. But the snacks were building the whole barn. Then she saw carefully pour the grape out the salad. She started to share the corn with the crab for it.\n",
            "\n",
            "The girl said thank you to the bunny and the mouse realised the food be ate with but just used to find the printer it. Then the bunny shared the mother was now now there again.Once upon a time, there was a little fast little boy named Jack. Jack always wanted to play golf.\n",
            "One day, his dad wanted to play his baseball. His Leo wanted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"A little girl went to the woods\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8-BmrtwkmGi",
        "outputId": "e45de44a-c79f-4634-90ac-08ba28d3a6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A little girl went to the woods. She saw a friendly bird in the wall! She was so happy and couldn't stop taking the little girl in the forest from the old ground. \n",
            "\n",
            "Soon, the little girl got in her want! She kept walking and watched a little girl and waved at the bracelet. Suddenly, she saw a man adding a picture. \n",
            "\n",
            "The little girl was feeling sad to see its camera and told the daughter what was at home. She had a big smile on his face and then the little girl invited everyone there. They became very good and the little girl and the little girl told the vendor candy he wanted to borrow the flag. The little girl thought the clown would have it to give it back forever. \n",
            "\n",
            "The little girl worked hard and before hungry as the old rock asked the worker the wise lady. The horse asked her friends if the man to unumpy son, so they took the car up to a salon and gave it the coins to the little girl.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
